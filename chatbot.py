import discord
from discord.ext import commands, tasks
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import quote_plus
import random
from llama_cpp import Llama
from collections import deque

bot_prompt = "You are a chatbot named kitty and 10-year old. And you are a developer. Please reply gently with English with Emoji.\n"
llm = Llama(model_path="./models/llama-2-13b-chat.Q5_K_M.gguf")
chat_log = deque()
log_maxlen = 3

# Store the last scraped news
last_scraped_title = ""

options = webdriver.ChromeOptions()
options.add_argument("--headless")
driver = webdriver.Chrome(options=options, service=ChromeService(ChromeDriverManager().install()))

# Set default intent for discord client
intents = discord.Intents.default()
intents.message_content = True
intents.typing = False
intents.presences = False

# Bot TOKEN, SERVER ID, CHANNEL ID
TOKEN = 'YOUR_TOKEN'
server_id = YOUR_SERVER_ID
channel_id = YOUR_CHANNEL_ID
bot = commands.Bot(command_prefix='/', intents=intents)

@bot.command()
async def chat(ctx, *, query):
    chatting = "\n".join(chat_log)
    current_chat = f"\nQ : {query}.\nA : "
    chatting += current_chat
    output = llm(bot_prompt+chatting, max_tokens=1024, stop=["Q : "], echo=False)
    answer = output["choices"][0]["text"]

    if len(chat_log) < log_maxlen:
        chat_log.append(current_chat+answer)
    else:
        chat_log.popleft()
        chat_log.append(current_chat+answer)

    await ctx.send(answer)

@bot.command()
async def paper(ctx, *, query):
    await ctx.send(f"ì•Œê² ìŠµë‹ˆë‹¤! {query}ì— ëŒ€í•œ ì•„ì¹´ì´ë¸Œ ë…¼ë¬¸ì„ ì°¾ì•„ë³¼ê²Œìš” ðŸ¤—")
    try:
        found = False
        baseUrl = 'https://www.google.com/search?q='

        if "paper" not in query:
            url = baseUrl + quote_plus(query+" paper")
        else:
            url = baseUrl + quote_plus(query)

        driver.get(url)
        driver.implicitly_wait(3)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # Extract search results
        result_links = []
        search_results = soup.select('.yuRUbf')
        if search_results:
            for result in search_results:
                result_title = result.find('h3')
                result_link = result.find('a')['href']
                if "arxiv.org" in result_link:
                    if result_link not in result_links:
                        result_links.append(result_link)
                        await ctx.send(f"### ì œëª©: {result_title.text}\n### ë§í¬: {result_link}")
                    found = True

            if not found:
                await ctx.send("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ ðŸ¥²") 
        else:
            await ctx.send("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ ðŸ¥²")
    
    except Exception as e:
        await ctx.send(f"An error occurred: {str(e)} ðŸ¥² ...")

@bot.event
async def on_ready():
    print(f'We have logged in as {bot.user}')
    server = discord.utils.get(bot.guilds, id=server_id)
    channel = discord.utils.get(server.text_channels, id=channel_id)
    # Start the news scraping task
    news_sender.start(channel)

@bot.event
async def on_message(message):
    if message.author.bot: # ë´‡ì´ ë³´ë‚¸ ë©”ì‹œì§€ì´ë©´ ë°˜ì‘í•˜ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤
        return
    
    if message.content.startswith('/'):  # Check if the message is a command
        await bot.process_commands(message)  # Process the command
        return  # Return early to avoid further processing

    if "ì•ˆë…•" in message.content:
        if random.random() < 0.3:
            await message.channel.send(message.author.display_name + "ë‹˜ ì•ˆë…•í•˜ì„¸ìš”!")
        elif random.random() < 0.6:
            await message.channel.send(message.author.display_name + "ë‹˜ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!")
        else:
            await message.channel.send(message.author.display_name + "ë‹˜ ë°˜ê°‘ìŠµë‹ˆë‹¤!")

    if "ì‹œë°œ" in message.content or "ì”¨ë°œ" in message.content or "ã……ã…‚" in message.content or "ã…ˆã„´" in message.content:
        await message.channel.send(message.author.display_name + "ë‹˜ ìš•ì€ ì¢‹ì§€ ì•Šì€ ìŠµê´€ìž…ë‹ˆë‹¤")

# Get new information from link per 0.1min (6 sec)
@tasks.loop(minutes=0.5)
async def news_sender(channel):
    global last_scraped_title
    new_scraped_news = scrape_news()

    # new_scraped newsë¡œë¶€í„° ì œëª© text ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê¸°
    new_scraped_title = new_scraped_news["topictitle"].find('h1').text
    if new_scraped_title != last_scraped_title:
        print(f"[New!] {new_scraped_title}")
        await send_news(channel, new_scraped_news, new_scraped_title)
        last_scraped_title = new_scraped_title
    
    # ë§Œì•½ ê¸°ì¡´ì— ê°€ì ¸ì™”ë˜ ë‰´ìŠ¤ëž‘ ê²¹ì¹˜ë©´ ë””ìŠ¤ì½”ë“œë¡œ ë³´ë‚´ì§€ ì•Šê³  ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ê¸°ë‹¤ë¦¼
    else:
        print("Keep waiting for another news ...")

# ë‰´ìŠ¤ ìŠ¤í¬ëž©í•˜ëŠ” í•¨ìˆ˜
def scrape_news():
    url = 'https://news.hada.io/new'
    baseurl = 'https://news.hada.io/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    news_container = soup.find(class_='topic_row')
    topictitle = news_container.find(class_='topictitle')
    topicdesc = news_container.find(class_='topicdesc')
    return {"baseurl": baseurl, "topictitle": topictitle, "topicdesc": topicdesc}

# ë””ìŠ¤ì½”ë“œë¡œ ë‰´ìŠ¤ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜
async def send_news(channel, news, title):
    # Extract the link within the 'a' tag
    link_desc = news["topicdesc"].find('a')['href'].strip()
    news_info = f'{"# "+title}\n- Geek News ë§í¬ : {news["baseurl"]+link_desc}'
    await channel.send(news_info)

bot.run(TOKEN)
driver.close()
